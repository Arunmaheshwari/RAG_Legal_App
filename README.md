# ğŸ§  RAG Chatbot for Legal Documents

A Retrieval-Augmented Generation (RAG) chatbot built using **LangChain**, **FAISS**, **Sentence Transformers**, and **Ollama** LLMs. Designed to help users query large legal documents with grounded, contextual answers.

---

## ğŸ“Œ Features

- ğŸ” **Contextual Retrieval**: Uses FAISS with semantic search for document chunking.
- ğŸ§  **Local LLMs**: Integrates `llama3`, `mistral`, or `tiny` via Ollama.
- ğŸ“„ **PDF Loader**: Parses and chunks legal PDFs into retrievable units.
- ğŸ’¬ **Streamlit Chat Interface**: Easy-to-use front-end for user interaction.
- âš™ï¸ **Environment Config**: Uses `.env` for easy config of embedding model and DB path.

---

## ğŸ§° Tech Stack

| Tool                | Purpose                         |
|---------------------|---------------------------------|
| `Streamlit`         | Chat UI                         |
| `LangChain`         | RAG pipeline orchestration      |
| `FAISS`             | Vector database                 |
| `sentence-transformers` | Embedding model (`MiniLM`) |
| `Ollama`            | Local LLM backend               |
| `pypdf`             | PDF parsing                     |
| `python-dotenv`     | Environment variable loading    |

---

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/Arunmaheshwari/RAG_Legal_App.git
cd RAG_Legal_App or RAG_Application
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```


### 3. Configure Environment

Create a .env file with:

```bash
EMBED_MODEL=BAAI/bge-small-en
DB_PATH=vectordb/faiss_index
LLM_MODEL = tinyllama:latest
```
ğŸ’¡ You can also use other models like llama3, mistral, etc., by updating the LLM_MODEL in the .env.


### 4. Start Ollama & Pull Model
```bash
ollama pull tinyllama or tinyllama:latest
ollama run tinyllama or tinyllama:latest

```


### 5. Run the App
```bash
streamlit run app.py
```


# ğŸ“ Project Structure

ğŸ“¦ Rag Legal document chatbot
â”œâ”€â”€ app.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ chunks/
â”‚   â””â”€â”€ chunks.py
â”œâ”€â”€ vectordb/
â”‚   â””â”€â”€ vectordf.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ollama_llm.py
â”‚   â”œâ”€â”€ prompt_template.py
â”‚   â””â”€â”€ retriever.py
â””â”€â”€ data/
    â””â”€â”€ AI Training Document.pdf


---

# ğŸ” Project Workflow
flowchart TD

    A[ğŸ“¥ Ingestion Phase] --> A1[ğŸ“„ Load PDF via PyPDF]
    A1 --> A2[âœ‚ï¸ Chunk Text using LangChain]
    A2 --> A3[ğŸ§  Generate Embeddings (MiniLM)]
    A3 --> A4[ğŸ’¾ Store Chunks + Vectors in FAISS DB]

    B[ğŸ“ Augmentation Phase] --> B1[ğŸ” User Query Input (via Streamlit)]
    B1 --> B2[ğŸ” Retrieve Relevant Chunks from FAISS]
    B2 --> B3[ğŸ“š Merge Context with User Query]

    C[ğŸ¤– Generation Phase] --> C1[ğŸ§  Pass Merged Context to Ollama LLM]
    C1 --> C2[ğŸ’¬ Generate Response]
    C2 --> C3[ğŸ“¤ Stream Output in Chat UI]

    A --> B
    B --> C


# ğŸ” Stages Explained
 - Ingestion: Load PDFs â†’ Chunk â†’ Embed â†’ Store in FAISS.

 - Augmentation: Take user query â†’ Retrieve relevant chunks.

 - Generation: Combine context + query â†’ Run through LLM â†’ Generate answer.

---



# âœï¸ Author
Arun Maheshwari